---
title: "STAT 226: Homework 8"
author: "Patrick Halkett"
fontsize: 11pt
output:
  pdf_document: default
  word_document: default
fig_crop: false
geometry: margin=0.75in
header-includes:
  - \usepackage{pdfpages}
---

```{r setup, include=FALSE}
library(MASS)
library(GGally)
library(openintro)
library(mosaic)
library(knitr)
library(tidyverse)
library(ggformula)
library(gridExtra)
library(broom)
options(width=70, digits=4, scipen=8)
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})
# Set R output size a bit smaller than default
opts_chunk$set(size='small') 
# Set the default to NOT display R code
opts_chunk$set(echo=FALSE) 
```


# Lab: Exploratory Analysis of the Wheat Data

```{r load-data}
wheat = read.csv("http://statistics.uchicago.edu/~collins/data/s226/wheat.csv")
```

#### Lab Exercise 1
From the side-by-side boxplot and histograms above,
do the 3 types of wheat kernels differ in the distribution of their density?

```{r exercise1}
# your answer to Exercise 1 goes here
```

#### Lab Exerecise 2
Make side-by-side boxplots or histograms to compare 
the distributions of the variable `weight` of the 3 types of kernels.
Repeat for `size`, `hardness`, and `moisture`.

```{r exercise2}
gf_histogram(~size|type, data=wheat, color="white") %>%
  gf_labs(x = "Size (microL)", 
          y = "Number of Wheat Kernels") + 
  facet_grid(type ~ .)
```

#### Lab Exercise 3

Make scatter plots for other pairs of explanatory variables, 
and use the color of points to indicate the `type` of kernels.
Is there any pair variables that seem to be highly correlated?
Discuss the relationship between variables and with `type`.

```{r exercise3}
gf_point(density~hardness, color=~type, data=wheat)
```

## Baseline-Category Logit Model

We fit a baseline-category logit model using the `type`
of kernels as response, and `density` as the only explanatory 
variable. We will use the `vglm` function in the `VGAM` library.

```{r VGAM-install, eval=FALSE}
# Run this line only if you have never installed `VGAM`
# Yes, you will need to run this once on rstudio.uchicago.edu
install.packages("VGAM")  
```

After installation, you will have to load the library.

```{r VGAM-load, message=FALSE, warning=FALSE}
library(VGAM)  
```

Now we can fit the baseline-category model.

```{r fit1}
fit1 =  vglm(type ~ density, data = wheat, family = multinomial)
```

The intercepts and slopes are
```{r fit1-coef}
coef(fit1)
a.h = coef(fit1)[1]
a.s = coef(fit1)[2]
b.h = coef(fit1)[3]
b.s = coef(fit1)[4]
data.frame(a.h, a.s, b.h, b.s)
```
So the R output above gives the equations for

log(P(`healthy`)/P(`Sprout`)) = $`r round(a.h, 2)` + `r round(b.h, 2)` \times$ `density`

and

log(P(`Scab`)/P(`Sprout`)) = $`r round(a.s, 2)` + (`r round(b.s, 2)`) \times$ `density`

## Exercise 4
Based on the model above, what are P(`healthy`), P(`Scab`),
and P(`Sprout`) respectively?

```{r exercise4}
# your answer to Exercise 4 goes here
```

```{r}
#HW Problem 1
wheat.grp = mutate(wheat, 
  weight.grp = cut(weight, breaks= c(5, 10, 15, 20, 25, 30, 35, 40, 45, 50)))
type.count = tally(weight.grp ~ type, data = wheat.grp)
addmargins(type.count, 2, FUN = list(Total = sum))

type.prop = prop.table(tally(weight.grp ~ type, data = wheat.grp), 1)
round(addmargins(type.prop, 2, FUN = list(Total = sum)), 3)

weight.grpmean = mean(weight~weight.grp, data = wheat.grp);
#weight.grpmean

plot(weight.grpmean, type.prop[,1], type="p", xlim=c(5,50),ylim=c(0,1.0),
     ylab="Estimated Probabilities", xlab="Weight (mg)")
points(weight.grpmean, type.prop[,2], pch=2)
points(weight.grpmean, type.prop[,3], pch=3)
#lines(weight.grpmean, type.prop[,2], lty=2)
#lines(weight.grpmean, type.prop[,3], lty=3)
legend("left",c("H","Sc","Sp"),pch=1:3, lty=1:3)

fit1 = vglm(type~weight, data=wheat, family=multinomial)
a.1 = coef(fit1)[1]
a.2 = coef(fit1)[2]
b.1 = coef(fit1)[3]
b.2 = coef(fit1)[4]

# P(Healthy)
curve(exp(a.1 + b.1*x) / (1 + exp(a.1 + b.1*x) + exp(a.2 + b.2*x)), lty = 1, add = T)

# P(Scab)
curve(exp(a.2 + b.2*x) / (1 + exp(a.1 + b.1*x) + exp(a.2 + b.2*x)), lty = 3, add = T)  

# P(Sprout)
curve(1 / (1 + exp(a.1 + b.1*x) + exp(a.2 + b.2*x)), lty = 2, add = T)

# Part e and more
fit2 = vglm(type~weight+density+hardness+size+moisture+class, data=wheat, family=multinomial)
summary(fit2)
#lrtest(fit2, "size")
#lrtest(fit2, c("size","moisture","class"))

#fit4 = vglm(type~weight+density+hardness, data=wheat, family=multinomial)
#summary(fit4)
```

```{r}
#Problem 2
Race = c("White","White","Black","Black") 
Gender = c("Female","Male","Female","Male") 
Yes = c(371,250,64,25) 
Undecided = c(49,45,9,5) 
No = c(74,71,15,13)

afterlife = matrix(c(Yes,Undecided,No), nrow=4)
dimnames(afterlife)=list(c("White Females", "White Males", "Black Females", "Black Males"),
                         c("Yes", "Undecided", "No"))
round(prop.table(afterlife,1),3)

fit3 =  vglm(cbind(Yes, Undecided, No) ~ Race + Gender, family = multinomial)

lrtest(fit3, "Race")
```

```{r}
#Problem 3
cumulative.logit = vglm(cbind(Yes, Undecided, No)~Race+Gender,family=cumulative(parallel=TRUE))
#pchisq(deviance(cumulative.logit),df=4,lower.tail=F)

#part goddamn e again
cumulative.logit2 = vglm(cbind(Yes, Undecided, No)~Race+Gender+Gender*Race,family=cumulative(parallel=TRUE))
#cumulative.logit2
lrtest(cumulative.logit2, cumulative.logit)
```

